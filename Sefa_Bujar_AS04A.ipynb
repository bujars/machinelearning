{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #Libray for creating it'a main object, Arrays + different functions for it\n",
    "import pandas as pd #Library to transform data, manipulate data\n",
    "import matplotlib.pyplot as plt #Library for visualizing data\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just self notes\n",
    "# Error = 1 - accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0    1    2    3               4\n",
      "0    5.1  3.5  1.4  0.2     Iris-setosa\n",
      "1    4.9  3.0  1.4  0.2     Iris-setosa\n",
      "2    4.7  3.2  1.3  0.2     Iris-setosa\n",
      "3    4.6  3.1  1.5  0.2     Iris-setosa\n",
      "4    5.0  3.6  1.4  0.2     Iris-setosa\n",
      "5    5.4  3.9  1.7  0.4     Iris-setosa\n",
      "6    4.6  3.4  1.4  0.3     Iris-setosa\n",
      "7    5.0  3.4  1.5  0.2     Iris-setosa\n",
      "8    4.4  2.9  1.4  0.2     Iris-setosa\n",
      "9    4.9  3.1  1.5  0.1     Iris-setosa\n",
      "10   5.4  3.7  1.5  0.2     Iris-setosa\n",
      "11   4.8  3.4  1.6  0.2     Iris-setosa\n",
      "12   4.8  3.0  1.4  0.1     Iris-setosa\n",
      "13   4.3  3.0  1.1  0.1     Iris-setosa\n",
      "14   5.8  4.0  1.2  0.2     Iris-setosa\n",
      "15   5.7  4.4  1.5  0.4     Iris-setosa\n",
      "16   5.4  3.9  1.3  0.4     Iris-setosa\n",
      "17   5.1  3.5  1.4  0.3     Iris-setosa\n",
      "18   5.7  3.8  1.7  0.3     Iris-setosa\n",
      "19   5.1  3.8  1.5  0.3     Iris-setosa\n",
      "20   5.4  3.4  1.7  0.2     Iris-setosa\n",
      "21   5.1  3.7  1.5  0.4     Iris-setosa\n",
      "22   4.6  3.6  1.0  0.2     Iris-setosa\n",
      "23   5.1  3.3  1.7  0.5     Iris-setosa\n",
      "24   4.8  3.4  1.9  0.2     Iris-setosa\n",
      "25   5.0  3.0  1.6  0.2     Iris-setosa\n",
      "26   5.0  3.4  1.6  0.4     Iris-setosa\n",
      "27   5.2  3.5  1.5  0.2     Iris-setosa\n",
      "28   5.2  3.4  1.4  0.2     Iris-setosa\n",
      "29   4.7  3.2  1.6  0.2     Iris-setosa\n",
      "..   ...  ...  ...  ...             ...\n",
      "120  6.9  3.2  5.7  2.3  Iris-virginica\n",
      "121  5.6  2.8  4.9  2.0  Iris-virginica\n",
      "122  7.7  2.8  6.7  2.0  Iris-virginica\n",
      "123  6.3  2.7  4.9  1.8  Iris-virginica\n",
      "124  6.7  3.3  5.7  2.1  Iris-virginica\n",
      "125  7.2  3.2  6.0  1.8  Iris-virginica\n",
      "126  6.2  2.8  4.8  1.8  Iris-virginica\n",
      "127  6.1  3.0  4.9  1.8  Iris-virginica\n",
      "128  6.4  2.8  5.6  2.1  Iris-virginica\n",
      "129  7.2  3.0  5.8  1.6  Iris-virginica\n",
      "130  7.4  2.8  6.1  1.9  Iris-virginica\n",
      "131  7.9  3.8  6.4  2.0  Iris-virginica\n",
      "132  6.4  2.8  5.6  2.2  Iris-virginica\n",
      "133  6.3  2.8  5.1  1.5  Iris-virginica\n",
      "134  6.1  2.6  5.6  1.4  Iris-virginica\n",
      "135  7.7  3.0  6.1  2.3  Iris-virginica\n",
      "136  6.3  3.4  5.6  2.4  Iris-virginica\n",
      "137  6.4  3.1  5.5  1.8  Iris-virginica\n",
      "138  6.0  3.0  4.8  1.8  Iris-virginica\n",
      "139  6.9  3.1  5.4  2.1  Iris-virginica\n",
      "140  6.7  3.1  5.6  2.4  Iris-virginica\n",
      "141  6.9  3.1  5.1  2.3  Iris-virginica\n",
      "142  5.8  2.7  5.1  1.9  Iris-virginica\n",
      "143  6.8  3.2  5.9  2.3  Iris-virginica\n",
      "144  6.7  3.3  5.7  2.5  Iris-virginica\n",
      "145  6.7  3.0  5.2  2.3  Iris-virginica\n",
      "146  6.3  2.5  5.0  1.9  Iris-virginica\n",
      "147  6.5  3.0  5.2  2.0  Iris-virginica\n",
      "148  6.2  3.4  5.4  2.3  Iris-virginica\n",
      "149  5.9  3.0  5.1  1.8  Iris-virginica\n",
      "\n",
      "[150 rows x 5 columns]\n",
      "0 [5.1 3.5 1.4 0.2 'Iris-setosa']\n",
      "1 [4.9 3.0 1.4 0.2 'Iris-setosa']\n",
      "2 [4.7 3.2 1.3 0.2 'Iris-setosa']\n",
      "3 [4.6 3.1 1.5 0.2 'Iris-setosa']\n",
      "4 [5.0 3.6 1.4 0.2 'Iris-setosa']\n",
      "5 [5.4 3.9 1.7 0.4 'Iris-setosa']\n",
      "6 [4.6 3.4 1.4 0.3 'Iris-setosa']\n",
      "7 [5.0 3.4 1.5 0.2 'Iris-setosa']\n",
      "8 [4.4 2.9 1.4 0.2 'Iris-setosa']\n",
      "9 [4.9 3.1 1.5 0.1 'Iris-setosa']\n",
      "10 [5.4 3.7 1.5 0.2 'Iris-setosa']\n",
      "11 [4.8 3.4 1.6 0.2 'Iris-setosa']\n",
      "12 [4.8 3.0 1.4 0.1 'Iris-setosa']\n",
      "13 [4.3 3.0 1.1 0.1 'Iris-setosa']\n",
      "14 [5.8 4.0 1.2 0.2 'Iris-setosa']\n",
      "15 [5.7 4.4 1.5 0.4 'Iris-setosa']\n",
      "16 [5.4 3.9 1.3 0.4 'Iris-setosa']\n",
      "17 [5.1 3.5 1.4 0.3 'Iris-setosa']\n",
      "18 [5.7 3.8 1.7 0.3 'Iris-setosa']\n",
      "19 [5.1 3.8 1.5 0.3 'Iris-setosa']\n",
      "20 [5.4 3.4 1.7 0.2 'Iris-setosa']\n",
      "21 [5.1 3.7 1.5 0.4 'Iris-setosa']\n",
      "22 [4.6 3.6 1.0 0.2 'Iris-setosa']\n",
      "23 [5.1 3.3 1.7 0.5 'Iris-setosa']\n",
      "24 [4.8 3.4 1.9 0.2 'Iris-setosa']\n",
      "25 [5.0 3.0 1.6 0.2 'Iris-setosa']\n",
      "26 [5.0 3.4 1.6 0.4 'Iris-setosa']\n",
      "27 [5.2 3.5 1.5 0.2 'Iris-setosa']\n",
      "28 [5.2 3.4 1.4 0.2 'Iris-setosa']\n",
      "29 [4.7 3.2 1.6 0.2 'Iris-setosa']\n",
      "30 [4.8 3.1 1.6 0.2 'Iris-setosa']\n",
      "31 [5.4 3.4 1.5 0.4 'Iris-setosa']\n",
      "32 [5.2 4.1 1.5 0.1 'Iris-setosa']\n",
      "33 [5.5 4.2 1.4 0.2 'Iris-setosa']\n",
      "34 [4.9 3.1 1.5 0.1 'Iris-setosa']\n",
      "35 [5.0 3.2 1.2 0.2 'Iris-setosa']\n",
      "36 [5.5 3.5 1.3 0.2 'Iris-setosa']\n",
      "37 [4.9 3.1 1.5 0.1 'Iris-setosa']\n",
      "38 [4.4 3.0 1.3 0.2 'Iris-setosa']\n",
      "39 [5.1 3.4 1.5 0.2 'Iris-setosa']\n",
      "40 [5.0 3.5 1.3 0.3 'Iris-setosa']\n",
      "41 [4.5 2.3 1.3 0.3 'Iris-setosa']\n",
      "42 [4.4 3.2 1.3 0.2 'Iris-setosa']\n",
      "43 [5.0 3.5 1.6 0.6 'Iris-setosa']\n",
      "44 [5.1 3.8 1.9 0.4 'Iris-setosa']\n",
      "45 [4.8 3.0 1.4 0.3 'Iris-setosa']\n",
      "46 [5.1 3.8 1.6 0.2 'Iris-setosa']\n",
      "47 [4.6 3.2 1.4 0.2 'Iris-setosa']\n",
      "48 [5.3 3.7 1.5 0.2 'Iris-setosa']\n",
      "49 [5.0 3.3 1.4 0.2 'Iris-setosa']\n",
      "50 [7.0 3.2 4.7 1.4 'Iris-versicolor']\n",
      "51 [6.4 3.2 4.5 1.5 'Iris-versicolor']\n",
      "52 [6.9 3.1 4.9 1.5 'Iris-versicolor']\n",
      "53 [5.5 2.3 4.0 1.3 'Iris-versicolor']\n",
      "54 [6.5 2.8 4.6 1.5 'Iris-versicolor']\n",
      "55 [5.7 2.8 4.5 1.3 'Iris-versicolor']\n",
      "56 [6.3 3.3 4.7 1.6 'Iris-versicolor']\n",
      "57 [4.9 2.4 3.3 1.0 'Iris-versicolor']\n",
      "58 [6.6 2.9 4.6 1.3 'Iris-versicolor']\n",
      "59 [5.2 2.7 3.9 1.4 'Iris-versicolor']\n",
      "60 [5.0 2.0 3.5 1.0 'Iris-versicolor']\n",
      "61 [5.9 3.0 4.2 1.5 'Iris-versicolor']\n",
      "62 [6.0 2.2 4.0 1.0 'Iris-versicolor']\n",
      "63 [6.1 2.9 4.7 1.4 'Iris-versicolor']\n",
      "64 [5.6 2.9 3.6 1.3 'Iris-versicolor']\n",
      "65 [6.7 3.1 4.4 1.4 'Iris-versicolor']\n",
      "66 [5.6 3.0 4.5 1.5 'Iris-versicolor']\n",
      "67 [5.8 2.7 4.1 1.0 'Iris-versicolor']\n",
      "68 [6.2 2.2 4.5 1.5 'Iris-versicolor']\n",
      "69 [5.6 2.5 3.9 1.1 'Iris-versicolor']\n",
      "70 [5.9 3.2 4.8 1.8 'Iris-versicolor']\n",
      "71 [6.1 2.8 4.0 1.3 'Iris-versicolor']\n",
      "72 [6.3 2.5 4.9 1.5 'Iris-versicolor']\n",
      "73 [6.1 2.8 4.7 1.2 'Iris-versicolor']\n",
      "74 [6.4 2.9 4.3 1.3 'Iris-versicolor']\n",
      "75 [6.6 3.0 4.4 1.4 'Iris-versicolor']\n",
      "76 [6.8 2.8 4.8 1.4 'Iris-versicolor']\n",
      "77 [6.7 3.0 5.0 1.7 'Iris-versicolor']\n",
      "78 [6.0 2.9 4.5 1.5 'Iris-versicolor']\n",
      "79 [5.7 2.6 3.5 1.0 'Iris-versicolor']\n",
      "80 [5.5 2.4 3.8 1.1 'Iris-versicolor']\n",
      "81 [5.5 2.4 3.7 1.0 'Iris-versicolor']\n",
      "82 [5.8 2.7 3.9 1.2 'Iris-versicolor']\n",
      "83 [6.0 2.7 5.1 1.6 'Iris-versicolor']\n",
      "84 [5.4 3.0 4.5 1.5 'Iris-versicolor']\n",
      "85 [6.0 3.4 4.5 1.6 'Iris-versicolor']\n",
      "86 [6.7 3.1 4.7 1.5 'Iris-versicolor']\n",
      "87 [6.3 2.3 4.4 1.3 'Iris-versicolor']\n",
      "88 [5.6 3.0 4.1 1.3 'Iris-versicolor']\n",
      "89 [5.5 2.5 4.0 1.3 'Iris-versicolor']\n",
      "90 [5.5 2.6 4.4 1.2 'Iris-versicolor']\n",
      "91 [6.1 3.0 4.6 1.4 'Iris-versicolor']\n",
      "92 [5.8 2.6 4.0 1.2 'Iris-versicolor']\n",
      "93 [5.0 2.3 3.3 1.0 'Iris-versicolor']\n",
      "94 [5.6 2.7 4.2 1.3 'Iris-versicolor']\n",
      "95 [5.7 3.0 4.2 1.2 'Iris-versicolor']\n",
      "96 [5.7 2.9 4.2 1.3 'Iris-versicolor']\n",
      "97 [6.2 2.9 4.3 1.3 'Iris-versicolor']\n",
      "98 [5.1 2.5 3.0 1.1 'Iris-versicolor']\n",
      "99 [5.7 2.8 4.1 1.3 'Iris-versicolor']\n",
      "100 [6.3 3.3 6.0 2.5 'Iris-virginica']\n",
      "101 [5.8 2.7 5.1 1.9 'Iris-virginica']\n",
      "102 [7.1 3.0 5.9 2.1 'Iris-virginica']\n",
      "103 [6.3 2.9 5.6 1.8 'Iris-virginica']\n",
      "104 [6.5 3.0 5.8 2.2 'Iris-virginica']\n",
      "105 [7.6 3.0 6.6 2.1 'Iris-virginica']\n",
      "106 [4.9 2.5 4.5 1.7 'Iris-virginica']\n",
      "107 [7.3 2.9 6.3 1.8 'Iris-virginica']\n",
      "108 [6.7 2.5 5.8 1.8 'Iris-virginica']\n",
      "109 [7.2 3.6 6.1 2.5 'Iris-virginica']\n",
      "110 [6.5 3.2 5.1 2.0 'Iris-virginica']\n",
      "111 [6.4 2.7 5.3 1.9 'Iris-virginica']\n",
      "112 [6.8 3.0 5.5 2.1 'Iris-virginica']\n",
      "113 [5.7 2.5 5.0 2.0 'Iris-virginica']\n",
      "114 [5.8 2.8 5.1 2.4 'Iris-virginica']\n",
      "115 [6.4 3.2 5.3 2.3 'Iris-virginica']\n",
      "116 [6.5 3.0 5.5 1.8 'Iris-virginica']\n",
      "117 [7.7 3.8 6.7 2.2 'Iris-virginica']\n",
      "118 [7.7 2.6 6.9 2.3 'Iris-virginica']\n",
      "119 [6.0 2.2 5.0 1.5 'Iris-virginica']\n",
      "120 [6.9 3.2 5.7 2.3 'Iris-virginica']\n",
      "121 [5.6 2.8 4.9 2.0 'Iris-virginica']\n",
      "122 [7.7 2.8 6.7 2.0 'Iris-virginica']\n",
      "123 [6.3 2.7 4.9 1.8 'Iris-virginica']\n",
      "124 [6.7 3.3 5.7 2.1 'Iris-virginica']\n",
      "125 [7.2 3.2 6.0 1.8 'Iris-virginica']\n",
      "126 [6.2 2.8 4.8 1.8 'Iris-virginica']\n",
      "127 [6.1 3.0 4.9 1.8 'Iris-virginica']\n",
      "128 [6.4 2.8 5.6 2.1 'Iris-virginica']\n",
      "129 [7.2 3.0 5.8 1.6 'Iris-virginica']\n",
      "130 [7.4 2.8 6.1 1.9 'Iris-virginica']\n",
      "131 [7.9 3.8 6.4 2.0 'Iris-virginica']\n",
      "132 [6.4 2.8 5.6 2.2 'Iris-virginica']\n",
      "133 [6.3 2.8 5.1 1.5 'Iris-virginica']\n",
      "134 [6.1 2.6 5.6 1.4 'Iris-virginica']\n",
      "135 [7.7 3.0 6.1 2.3 'Iris-virginica']\n",
      "136 [6.3 3.4 5.6 2.4 'Iris-virginica']\n",
      "137 [6.4 3.1 5.5 1.8 'Iris-virginica']\n",
      "138 [6.0 3.0 4.8 1.8 'Iris-virginica']\n",
      "139 [6.9 3.1 5.4 2.1 'Iris-virginica']\n",
      "140 [6.7 3.1 5.6 2.4 'Iris-virginica']\n",
      "141 [6.9 3.1 5.1 2.3 'Iris-virginica']\n",
      "142 [5.8 2.7 5.1 1.9 'Iris-virginica']\n",
      "143 [6.8 3.2 5.9 2.3 'Iris-virginica']\n",
      "144 [6.7 3.3 5.7 2.5 'Iris-virginica']\n",
      "145 [6.7 3.0 5.2 2.3 'Iris-virginica']\n",
      "146 [6.3 2.5 5.0 1.9 'Iris-virginica']\n",
      "147 [6.5 3.0 5.2 2.0 'Iris-virginica']\n",
      "148 [6.2 3.4 5.4 2.3 'Iris-virginica']\n",
      "149 [5.9 3.0 5.1 1.8 'Iris-virginica']\n",
      "['Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica']\n"
     ]
    }
   ],
   "source": [
    "# read Iris data set\n",
    "\n",
    "#Take the CSV and transform it into a Dataframe --> A table. \n",
    "#df = data file\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header = None)\n",
    "#df = pd.read_csv('iris.csv.xlsx', header = None)\n",
    "df.tail()\n",
    "\n",
    "print(df)\n",
    "\n",
    "#''' Store the data into integer indexed array : all rows : all columns . IA = Indexed Array  '''\n",
    "IA = df.iloc[:,:].values\n",
    "for i_ in range(0,len(df),1):\n",
    "    print(i_, IA[i_,:])\n",
    "\n",
    "\n",
    "#''' Store only the last column, The iris type. CL == Column Last '''\n",
    "Cl = IA[:,4] \n",
    "print(Cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']\n"
     ]
    }
   ],
   "source": [
    "# get Iris classifications\n",
    "\n",
    "#Find the unique values. Ie, the three types of Iris. IC = Iris Column\n",
    "IC = np.unique(IA[:,4])\n",
    "print (IC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris-setosa red o\n",
      "Iris-versicolor blue x\n",
      "Iris-virginica green *\n"
     ]
    }
   ],
   "source": [
    "pltC = ('red', 'blue', 'green') #Array of Plot Colors\n",
    "pltM = ('o', 'x', '*') #Array of Plot symbol\n",
    "\n",
    "#Associate a classification with a color and a symbol\n",
    "for i in range(0,3):\n",
    "    print(IC[i], pltC[i], pltM[i])\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length in cm\n",
      "sepal width in cm\n",
      "petal length in cm\n",
      "petal width in cm\n"
     ]
    }
   ],
   "source": [
    "# setup Iris feature lablels\n",
    "\n",
    "#Iris F = Iris Features\n",
    "IrisF=(\"sepal length in cm\", \"sepal width in cm\", \"petal length in cm\", \"petal width in cm\")\n",
    "for i in range(0,4):\n",
    "    print(IrisF[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.0 1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.0 3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.0 3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.0 1.4 0.1]\n",
      " [4.3 3.0 1.1 0.1]\n",
      " [5.8 4.0 1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.0 0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.0 3.0 1.6 0.2]\n",
      " [5.0 3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.0 3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [4.4 3.0 1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.0 3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.0 3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.0 1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.0 3.3 1.4 0.2]\n",
      " [7.0 3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.0 1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1.0]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.0 2.0 3.5 1.0]\n",
      " [5.9 3.0 4.2 1.5]\n",
      " [6.0 2.2 4.0 1.0]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.0 4.5 1.5]\n",
      " [5.8 2.7 4.1 1.0]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.0 1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.0 4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.0 5.0 1.7]\n",
      " [6.0 2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1.0]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1.0]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.0 2.7 5.1 1.6]\n",
      " [5.4 3.0 4.5 1.5]\n",
      " [6.0 3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.0 4.1 1.3]\n",
      " [5.5 2.5 4.0 1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.0 4.6 1.4]\n",
      " [5.8 2.6 4.0 1.2]\n",
      " [5.0 2.3 3.3 1.0]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.0 4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.0 1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.0 2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.0 5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.0 5.8 2.2]\n",
      " [7.6 3.0 6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2.0]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.0 5.5 2.1]\n",
      " [5.7 2.5 5.0 2.0]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.0 5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.0 2.2 5.0 1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2.0]\n",
      " [7.7 2.8 6.7 2.0]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.0 1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.0 4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.0 5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2.0]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.0 6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.0 3.0 4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.0 5.2 2.3]\n",
      " [6.3 2.5 5.0 1.9]\n",
      " [6.5 3.0 5.2 2.0]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.0 5.1 1.8]]\n",
      "['Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica']\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "features = IA[:, 0:4]\n",
    "classificationsLabels = IA[:, 4]\n",
    "print(features)\n",
    "print(classificationsLabels)\n",
    "\n",
    "# Converting labels to numeric values. \n",
    "classificationValues = np.zeros(len(classificationsLabels))\n",
    "print(classificationValues)\n",
    "\n",
    "for i in range(0, len(classificationValues)):\n",
    "    if(classificationsLabels[i] == 'Iris-setosa'):\n",
    "        classificationValues[i] = 0\n",
    "    elif(classificationsLabels[i] == 'Iris-versicolor'):\n",
    "        classificationValues[i] = 1\n",
    "    elif(classificationsLabels[i] == 'Iris-virginica'):\n",
    "        classificationValues[i] = 2\n",
    "print(classificationValues)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(features, classificationValues, test_size=0.50, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_clf = svm.SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_clf.fit(X_Train, Y_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear_clf.support_vectors_ #The samples (values) that make up the support vector points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_clf.support_ #The indicies of all of the support vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_clf.n_support_ #==> Show the number of support vectors per classificaiton "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9733333333333334\n",
      "0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "linear_training_error = linear_clf.score(X_Train, Y_Train) #Compute the mean accuracy for training data\n",
    "linear_testing_error = linear_clf.score(X_Test, Y_Test) # Compute the mean accuracy for testing data\n",
    "print(linear_training_error)\n",
    "print(linear_testing_error)\n",
    "      \n",
    "#Accuracys are pretty similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 1. 0. 2. 0. 2. 0. 1. 1. 1. 2. 1. 1. 1. 1. 0. 1. 1. 0. 0. 2. 1. 0. 0.\n",
      " 2. 0. 0. 1. 1. 0. 2. 1. 0. 2. 2. 1. 0. 2. 1. 1. 2. 0. 2. 0. 0. 1. 2. 2.\n",
      " 1. 2. 1. 2. 1. 1. 2. 2. 2. 2. 1. 2. 1. 0. 2. 1. 1. 1. 1. 2. 0. 0. 2. 1.\n",
      " 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Let's look into the accuracy per classification (Instead of total/overall)\n",
    "# We will look at the accuracy between the Testing data!\n",
    "\n",
    "linear_Y_Predict = linear_clf.predict(X_Test)\n",
    "print(linear_Y_Predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "#Import scikit-learn metrics module for accuracy calculation\\n\",\n",
    "from sklearn import metrics\n",
    "\n",
    "# Confirm score is the same as above for testing data (As it should be)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(Y_Test, linear_Y_Predict))\n",
    "# It is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00        21\n",
      "        1.0       0.97      0.97      0.97        30\n",
      "        2.0       0.96      0.96      0.96        24\n",
      "\n",
      "avg / total       0.97      0.97      0.97        75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_Test, linear_Y_Predict))\n",
    "\n",
    "#Focusing on just precision, we can see that setosa is the most correctly classified, then versicolor, then viriginica. \n",
    "#With a decrease in precision\n",
    "\n",
    "#While this model is different from Adeline, remmeber that we had the mos taccuracy there, \n",
    "#for setosa becasue it was more defined/seperated by its features. \n",
    "#Having lower sepall widht/length sepal values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we create a polynomial kernal SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_clf = svm.SVC(kernel='poly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='poly',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polynomial_clf.fit(X_Train, Y_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "polynomial_training_accuracy = polynomial_clf.score(X_Train, Y_Train) #Compute the mean accuracy for training data\n",
    "polynomial_testing_accuracy = polynomial_clf.score(X_Test, Y_Test) # Compute the mean accuracy for testing data\n",
    "print(polynomial_training_accuracy)\n",
    "print(polynomial_testing_accuracy)\n",
    "      \n",
    "#Accuracy for training data is almost perfect. Accuracy for testing data is approximately the same as linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 1. 0. 2. 0. 2. 0. 1. 1. 1. 2. 1. 1. 1. 1. 0. 1. 1. 0. 0. 2. 1. 0. 0.\n",
      " 2. 0. 0. 1. 1. 0. 2. 1. 0. 2. 2. 1. 0. 2. 1. 1. 2. 0. 2. 0. 0. 1. 2. 2.\n",
      " 2. 2. 1. 2. 1. 1. 2. 1. 2. 2. 1. 2. 1. 0. 2. 1. 1. 1. 1. 2. 0. 0. 2. 1.\n",
      " 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Let's look into the accuracy per classification (Instead of total/overall)\n",
    "# We will look at the accuracy between the Testing data!\n",
    "\n",
    "polynomial_Y_Predict = polynomial_clf.predict(X_Test)\n",
    "print(polynomial_Y_Predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "#Import scikit-learn metrics module for accuracy calculation\\n\",\n",
    "from sklearn import metrics\n",
    "\n",
    "# Confirm score is the same as above for testing data (As it should be)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(Y_Test, polynomial_Y_Predict))\n",
    "# It is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00        21\n",
      "        1.0       0.97      0.97      0.97        30\n",
      "        2.0       0.96      0.96      0.96        24\n",
      "\n",
      "avg / total       0.97      0.97      0.97        75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_Test, polynomial_Y_Predict))\n",
    "\n",
    "#Focusing on just precision, we can see that setosa is the most correctly classified, then versicolor, then viriginica. \n",
    "#With a decrease in precision\n",
    "\n",
    "#While this model is different from Adeline, remmeber that we had the mos taccuracy there, \n",
    "#for setosa becasue it was more defined/seperated by its features. \n",
    "#Having lower sepall widht/length sepal values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overall, we notice very similar accuracies.\n",
    "# The only difference here is how the training data was more accuractly fit that in the linear model. \n",
    "\n",
    "\n",
    "#Polynomial Kernels however, do allow us to better fit the data (Or a polynomial function) Hence why we notice a 1.0 fit for the training data,\n",
    "#However our testing data stillis off a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with Kernal RBF (Good for non-linearly seperable data. It tries to group them however can overfit -- for small data sets.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_clf = svm.SVC(kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbf_clf.fit(X_Train, Y_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9733333333333334\n",
      "0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "rbf_training_accuracy = rbf_clf.score(X_Train, Y_Train) #Compute the mean accuracy for training data\n",
    "rbf_testing_accuracy = rbf_clf.score(X_Test, Y_Test) # Compute the mean accuracy for testing data\n",
    "print(rbf_training_accuracy)\n",
    "print(rbf_testing_accuracy)\n",
    "      \n",
    "#Accuracy for training data is approzimately the same as that of linear\n",
    "# Accuracy for testing data is much lower than all previous models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 1. 0. 2. 0. 2. 0. 1. 1. 1. 2. 1. 1. 1. 1. 0. 1. 1. 0. 0. 2. 1. 0. 0.\n",
      " 1. 0. 0. 1. 1. 0. 2. 1. 0. 2. 2. 1. 0. 2. 1. 1. 2. 0. 2. 0. 0. 1. 2. 2.\n",
      " 1. 2. 1. 2. 1. 1. 2. 1. 1. 2. 1. 2. 1. 0. 2. 1. 1. 1. 1. 2. 0. 0. 2. 1.\n",
      " 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Let's look into the accuracy per classification (Instead of total/overall)\n",
    "# We will look at the accuracy between the Testing data!\n",
    "\n",
    "rbf_Y_Predict = rbf_clf.predict(X_Test)\n",
    "print(rbf_Y_Predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "#Import scikit-learn metrics module for accuracy calculation\\n\",\n",
    "from sklearn import metrics\n",
    "\n",
    "# Confirm score is the same as above for testing data (As it should be)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(Y_Test, rbf_Y_Predict))\n",
    "# It is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00        21\n",
      "        1.0       0.88      0.97      0.92        30\n",
      "        2.0       0.95      0.83      0.89        24\n",
      "\n",
      "avg / total       0.94      0.93      0.93        75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_Test, rbf_Y_Predict))\n",
    "\n",
    "#Focusing on just precision, we can see that setosa is the most correctly classified, then versicolor, then viriginica. \n",
    "#With a decrease in precision\n",
    "\n",
    "#While this model is different from Adeline, remmeber that we had the mos taccuracy there, \n",
    "#for setosa becasue it was more defined/seperated by its features. \n",
    "#Having lower sepall widht/length sepal values.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice how here we see the least accuracy for the classification label of 1.0\n",
    "# and a decrease of 2.0 and overall score\n",
    "\n",
    "\n",
    "# What's the explanation for this. Well from my brief understanding of RBF is that it tries to group them more alined than just by a line.\n",
    "#However if we recall from the iris dataset, many points of Versicolor + Virginica cross over, so the model may be overfitting too much\n",
    "#and thus decreasing the margin/space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearSVC_clf = svm.LinearSVC(C=1.0, max_iter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=10000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linearSVC_clf.fit(X_Train, Y_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9733333333333334\n",
      "0.92\n"
     ]
    }
   ],
   "source": [
    "linearSVC_training_accuracy = linearSVC_clf.score(X_Train, Y_Train) #Compute the mean accuracy for training data\n",
    "linearSVC_testing_accuracy = linearSVC_clf.score(X_Test, Y_Test) # Compute the mean accuracy for testing data\n",
    "print(linearSVC_training_accuracy)\n",
    "print(linearSVC_testing_accuracy)\n",
    "      \n",
    "#Accuracy for training data is approzimately the same as that of linear\n",
    "# Accuracy for testing data is much lower than all previous models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 1. 0. 2. 0. 2. 0. 1. 1. 1. 2. 1. 1. 1. 1. 0. 1. 1. 0. 0. 2. 2. 0. 0.\n",
      " 2. 0. 0. 1. 1. 0. 2. 2. 0. 2. 2. 1. 0. 2. 1. 1. 2. 0. 2. 0. 0. 1. 2. 2.\n",
      " 1. 2. 1. 2. 2. 1. 2. 2. 2. 2. 1. 2. 2. 0. 2. 1. 1. 1. 1. 2. 0. 0. 2. 1.\n",
      " 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Let's look into the accuracy per classification (Instead of total/overall)\n",
    "# We will look at the accuracy between the Testing data!\n",
    "\n",
    "linearSVC_Y_Predict = linearSVC_clf.predict(X_Test)\n",
    "print(linearSVC_Y_Predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "#Import scikit-learn metrics module for accuracy calculation\\n\",\n",
    "from sklearn import metrics\n",
    "\n",
    "# Confirm score is the same as above for testing data (As it should be)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(Y_Test, linearSVC_Y_Predict))\n",
    "# It is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00        21\n",
      "        1.0       0.96      0.83      0.89        30\n",
      "        2.0       0.82      0.96      0.88        24\n",
      "\n",
      "avg / total       0.93      0.92      0.92        75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(Y_Test, linearSVC_Y_Predict))\n",
    "#Focusing on just precision, we can see that setosa is the most correctly classified, then versicolor, then viriginica. \n",
    "#With a decrease in precision\n",
    "\n",
    "#While this model is different from Adeline, remmeber that we had the mos taccuracy there, \n",
    "#for setosa becasue it was more defined/seperated by its features. \n",
    "#Having lower sepall widht/length sepal values.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Linear: 0.9733333333333334\n",
      "Accuracy Poly: 0.9733333333333334\n",
      "Accuracy RBF: 0.9333333333333333\n",
      "Accuracy AdjustesdLinear: 0.92\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Linear:\",metrics.accuracy_score(Y_Test, linear_Y_Predict))\n",
    "print(\"Accuracy Poly:\",metrics.accuracy_score(Y_Test, polynomial_Y_Predict))\n",
    "print(\"Accuracy RBF:\",metrics.accuracy_score(Y_Test, rbf_Y_Predict))\n",
    "print(\"Accuracy AdjustesdLinear:\",metrics.accuracy_score(Y_Test, linearSVC_Y_Predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "To summerize my results.\n",
    "\n",
    "For this assignment, we had to model Support Vector Classifiers with different kernels.\n",
    "The dataset that we used is the iris dataset.\n",
    "From previous assingments + discussions, one thing that we know is that the iris dataset is not linearly seperable \n",
    "(As we saw in the plots.) \n",
    "The Iris dataset has 3 classifications Setosa, Versicolor, and Viriginica, \n",
    "and four features: Sepal Lenght, Sepal Width, Petal Lenght, Petal Width.\n",
    "    \n",
    "For the Support Vector Classifiers we passed in all 3 classification and 4 features.\n",
    "The data used was first split into 50 percent training data and 50 percent testing data. \n",
    "(Value is chosen to be consistent with previous assingments)\n",
    "\n",
    "The four Kernels that were modeled are linear, polynomial, radial or rbf, and linear which minimizes the squared hinge loss and uses OVA instead of OAO reduction.\n",
    "\n",
    "As initial hypothesis was that Polynomial and RBF kernels are to produce more accurate models, as they\n",
    "are intended to work for non-linearly seperatable data. \n",
    "The polynomial kernal seeks a mapping function of degree 3 (The number of features - 1)\n",
    "and the RBF seeks to group regions, or informally the neighbors around it and group them.\n",
    "The linear kernels, of course seek to find lines that divide the three classes. \n",
    "\n",
    "\n",
    "After running the four models, we notice that Polynomial indeed has the greatest accuracy, of 0.97 on the test data.\n",
    "After that, follows the ordinary linear of 0.97.\n",
    "To our surprise, rbf (which is the most commonly used kernel) had a low accuracy of 0.93. \n",
    "Finally, 'adjusted' linear has the lowest accuracy of 0.92.\n",
    "\n",
    "If we observe the score on the training data (found in the preceeding cells)\n",
    "we will notice that the order from greatest to least acuracy on the training \n",
    "data follows polynomial, linear, rbf, and adjusted linear.\n",
    "\n",
    "Thus, the Support vector classifier model that best fits the iris dataset \n",
    "is the one with a Polynomial Kernal.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
